
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>Representation Learning | Skin Cancer</title>
        <link href="style.css" rel="stylesheet" />

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5.2/jquery.min.js" type="text/javascript"></script>
        <script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.8.9/jquery-ui.min.js" type="text/javascript"></script>

        <!-- Mathjax -->
        <script type="text/x-mathjax-config">
            // enables inline symbols
            MathJax.Hub.Config({
tex2jax: {
inlineMath: [ ['$','$'], ["\\(","\\)"] ],
processEscapes: true
}
});
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


        <script>
            let loadFunction = function(){
                document.getElementById('app').style.display = 'block';
                document.getElementById('loading').style.display = 'none';
            }
        </script>
    </head>
    <body onload='loadFunction()'>

        <div id="loading">
            <div class='loader'></div>
        </div>

        <div id="app">
            <div id="header" style="height: 60px; background-color: white; width: 100%;">
                <img src="imgs/CSIRO.jpeg" height="50" style="padding: 5px">

            </div>

            <div id="popup">
                <div id='inner_pop'>
                    <span>
                        For a smoother interactive experience, it is recommended that you download the repository and view it locally.
                        <br>
                        <br>
                        <a download="RepresentationLearningNonMelanoma.zip" href="https://github.com/smthomas-sci/RepresentationLearningNon-Melanoma/archive/refs/heads/main.zip">[DOWNLOAD ~266MB]</a>
                        <br>
                    </span>
                    <br>
                    <div id="okay" onclick="okay()">
                        Close
                    </div>
                    <script>
                        let okay = function(){
                            let div = document.getElementById("popup");
                            div.style.display = "none";
                            document.body.style.overflow = "initial";
                        }
                    </script>
                </div>
            </div>

            <div style="background: white; max-width: 700px; margin: auto">
                <h1>Representation Learning for Non-Melanoma Skin Cancer</h1>

                <p style="color: gray">S.M.Thomas | 5th September 2022</p>
                <p style="color: #0ba0cd;">Australian e-Health Research Centre, CSIRO</p>

                <p style='font-size: 0.8em;'>This article explores ideas presented in the paper <a href="https://arxiv.org/abs/2209.01779">Representation Learning for Non-Melanoma Skin Cancer using Latent Autoencoders</a>.
                    The accompanying code is available in the Github <a href="https://github.com/smthomas-sci/RepresentationLearningNon-Melanoma">repository</a>.

                    <br><br>
                    <span style="font-weight: bold;">
                        Note: For a smoother interactive experience, it is recommended that you download the repository and view it locally.
                        <br>
                        <a download="RepresentationLearningNonMelanoma.zip" href="https://github.com/smthomas-sci/RepresentationLearningNon-Melanoma/archive/refs/heads/main.zip">[DOWNLOAD ~266MB]</a>
                    </span>
                </p>

                <hr>
                <br>

                <h2>Useful Representations of Non-Melanoma Skin Cancer</h2>

                <p>
                    Generative modelling techniques such as GANs, Diffusion Models or Autoregressive Transformers, have
                    shown unprecedented representational capacity. They take high-dimensional data inputs from a particular problem domain (e.g. images, text
                    or both), and learn a  lower-dimensional represention which captures semantic structure and approximates
                    the real world distribution. We can then sample from the distribution, generating synethic images
                    or texts which resmeble real-world data. However, merely drawing samples from the distribution is limited in its application
                    to real-world problems. Indeed, a desirable ability is to be able to project existing data points into a structured latent
                    space, not one only optimized for discriminatory tasks such as classification. Why this is desriable is arguably not-obvious
                    and also under-appreciated, particularly for high-stakes decision domains such as medical imaging.
                    Therefore, this work attempts to showcase several ways in which learning to generate real images can (in the long term) improve
                    the quality of our models and also deliver highly interpretable outputs. This work focuses on images and text within the context of digital pathology,
                    but the techniques are applicable to other medical (and non-medical) domains with multi-modal data.
                </p>

                <p>
                    Digital pathology utilises microscopic images of tissues and cells at various magnifications, where the morphological features
                    are visualy enhanced using H&E staining (pink and purple spectrum of colours). In this case, images of skin tissue
                    were used, representing healthly tissue and cancerous tissue (Intra-epidermal Carcinoma - IEC), and the graduations in between. The data
                    consisted of 11,588 images of size $256 \times 256$ pixels, each with an accompanying natural language description using a controlled
                    vocabularly of anatomical pathology terms.
                </p>

                <br>

                <div>
                    <div style="display: inline-block; width: 400px; vertical-align: top">
                        <br>
                        <br>
                        <p>Each image captures three layers of the skin.</p>
                        <ul>
                            <li class='img_hover' id="hover_ker">Keratin Layer</li>
                            <li class='img_hover' id="hover_epi">Epidermal Layer</li>
                            <li class='img_hover' id="hover_der">Dermal Layer</li>
                        </ul>
                    </div>
                    <img id="iec_example" src="imgs/iec_regions/all.jpg">

                    <script src="./hover.js"></script>
                </div>

                <p>The accompanying captions described the above layers in a systematic way e.g..</p>

                <p style="font-style: italic; margin: 10px auto 10px auto; width: 550px; font-family: Courier New;">The upper layer shows fragmented basket weave keratosis with focal parakeratosis.
                    The epidermis shows severe dysplasia. The dermis shows inflammation.</p>

                <p>
                    Instead of using a traditional Generative Adversarial Network (GAN) training paradigm, which <i>implicitly</i> learns to match a target distribution,
                    the Adversarial Latent Autoencoder (ALAE) paradigm is used. This is a slight modification which includes an additional term to <i>explicitly</i>
                    match the target distiribution via an autoencoding loss. To improve reconstruction quality, a subnetwork, consisting of
                    an $\text{encoder}$ and $\text{decoder}$ network, is first pre-trained for image reconstruction. The adversarial training then begins using
                    a locked network, where only the latent represention, $w$, is learned. [Refer to the paper for details].
                </p>

                <p>
                    The video below shows the training progress of the adversarial training stage. The model learns the global structure of a highly-diverse
                    set of histological images rather quickly, with the majority of training dedicated to learning finer and finer detail.
                    The model captures variations in all three tissue layers, including staining and background colours. Although it may look just a like traditional autoencoder,
                    the adversarial component contrains the model so that all inputs are placed in a structured latent space $w$, capturing the relationships between
                    features within the images.
                </p>

            </div>

            <div style="width: 100%; margin: 50px auto 50px auto; text-align: center; background: white;">
                <video src="imgs/training_progress.mp4" controls loop height="200" style="margin: auto">
            </div>


            <div style="width: 100%; margin: auto; background: white;">
                <div style="max-width: 700px; margin: auto;">

                    <h2 id="diverseSamples">Diverse Samples and Interpolations</h2>
                    <p>
                        We can demonstrate the learned representation clearly by <i>sampling</i> from the latent space (via a mapping network $F$, such as that $w=F(x)$, where $z \sim N(0, 1)$. Below you can
                        see diverse samples of <i>synthetic</i> images, and their interpolations between different points within $w$. The result is that every point within $w$ correponds to a
                        real, or seemingly (conjectured) real image. Indeed, we can directly see for ourselves how a healthy tissue <i>transforms</i> into a cancerous tissue
                        ( according to how the model understands the world).
                    </p>

                </div>

                <div style="text-align: center;">
                    <video src="imgs/sample_interpolations_6x6.mp4" controls loop height="600" style="margin: auto">
                </div>

            </div>

            <div style="background-color: white; padding: 75px; margin:auto; max-width: 700px;">
                <h2>Concept Vectors</h1>

                <p>
                    We can refine the way that we explore the structure of the latent space using labels associated with images. Indeed, it is a reasonable assumption that we would already have labels for
                    real data, whether dense in the case of text, or sparse in the case of class labels. We can use the labels to define concept vectors, describing directions within the latent space $w$
                    that correspond to intentional semantic manipulations of the image content. Examples of this can be seen below, where images are transformed by moving their location in $w$-space along a particular
                    concept e.g. increased inflammation or increased dysplasia.
                </p>





            </div>


            <section style="background-color: white; margin:auto; width: 100%; text-align: center;">

                <div class='concept_slider'>
                    <img id="basket" src="imgs/concepts/basketweave/025.jpg">
                    <br>

                    <span class='slider_label'>Parakeratosis</span>
                    <input type="range" min="0" max="50" oninput="basket_update(this)">
                    <span class='slider_label'>Basket weave</span>

                    <script>
                        let basket = document.getElementById("basket");
                        let basket_update = function(el){
                            basket.src = "./imgs/concepts/basketweave/" + el.value.padStart(3, 0) + '.jpg'
                        }
                    </script>
                </div>

                <div class='concept_slider'>
                    <img id="dysplasia" src="imgs/concepts/dysplasia/025.jpg">
                    <br>

                    <span class='slider_label'>+ Dysplasia</span>
                    <input type="range" min="0" max="50" oninput="dysplasia_update(this)">
                    <span class='slider_label'>- Dysplasia</span>

                    <script>
                        let dysplasia = document.getElementById("dysplasia");
                        let dysplasia_update = function(el){
                            dysplasia.src = "./imgs/concepts/dysplasia/" + el.value.padStart(3, 0) + '.jpg'
                        }
                    </script>
                </div>


                <div class='concept_slider'>
                    <img id="inflame" src="imgs/concepts/inf/000.jpg">
                    <br>

                    <span class='slider_label'>- Inflammation</span>
                    <input type="range" min="0" max="23" value="0" oninput="inflame_update(this)">
                    <span class='slider_label'>+ Inflammation</span>

                    <script>
                        let inflame = document.getElementById("inflame");
                        let inflame_update = function(el){
                            inflame.src = "./imgs/concepts/inf/" + el.value.padStart(3, 0) + '.jpg'
                        }
                    </script>
                </div>


                <div class='concept_slider'>
                    <img id="solar" src="imgs/concepts/solar/020.jpg">
                    <br>

                    <span class='slider_label'>- Solar Elastosis</span>
                    <input type="range" min="0" max="40" value="20" oninput="solar_update(this)">
                    <span class='slider_label'>+ Solar Elastosis</span>

                    <script>
                        let solar = document.getElementById("solar");
                        let solar_update = function(el){
                            solar.src = "./imgs/concepts/solar/" + el.value.padStart(3, 0) + '.jpg'
                        }
                    </script>
                </div>

                <div style="background-color: white; padding: 75px; margin:auto; max-width: 700px; text-align: left;">

                    <p>
                        The above shows that we can use simple linear directions to characterise images in terms of high-level concepts along a continuum. With this ability,
                        we can work towards characterising <i>cancer as a progression along a continuum</i>, rather than just binary. This has enormous potential for systematic and repeatable characterisation
                        of cancer, alleviating the discrepancies associated with inter and intra-observer variable in pathologists (Lewis et al., 2015).
                    </p>


                    <p>
                        The features in the images above retain many correlations e.g. increased inflammation correlates
                        with increased dysplasia. In reality, such features have more independence. Consequently, as models improve and
                        features in the latent space are further disentangled (e.g. using a Style Network (Wu et al., 2021) ), the value for this method to
                        produced highly interpretable outputs and the potential for finer and finer separation of the tissue progression increases.
                    </p>

                </div>


            </section>

            <section>


                <div style="max-width: 700px; margin: auto;">


                    <h2>Exhaustive Latent Space Exploration</h2>

                    <p>
                        The fact that we can sample from the latent space also helps to put bounds on what is and is not a realistic point in space. For example, one approach is to learn a 2-dimensional
                        representation of both the <span class="tag" style="background: #ededed;">sampled (synthetic) images</span>, and the <span class="tag" style="background: #890da1; color: white;">real images</span>,
                        and see where they do and do not overlap.  Exhaustive sampling reveals what every
                        single point looks like, as well as it's closet real data point, allowing proability estimates to be estabilished for a given region. This technique also reveals how the densities
                        of the sampled and real distributions compare. Intuitively, images that fall outside of the "cloud of realism" can be easily identfied and visualised.
                    </p>


                    <style>

.tag {
    padding: 0px 8px 0px 8px;
    margin: 2px;
    border: 1px solid black;
    border-radius: 5px;
}

                    </style>

                </div>

                <div height="400" style="background-color: white; padding: 75px;">
                    <img src="imgs/latent_space.gif" style="display: block; margin: auto;">
                </div>

            </section>


            <section style="background-color: black;">

                <div class="main">

                    <div style="color: white; max-width: 700px; margin: auto;">

                        <br>
                        <br>
                        <br>
                        <h2>Highly Expressive Characterisation</h2>

                        <p>
                            Pathologists have created a highly specialised vocabulary to describe all the nuanced varations they see under the microscope. There is
                            thus a natural pairing of images and words. When combined with text generation models (in this case an auto-regressive transformer), we
                            can work towards a highly-expressive characterisation of the entire latent space, using the nuanced language that pathologist themselves
                            use. This firstly provides a way for us to criticise the knowledge within the model e.g. errors between images and their captions*, but,
                            secondly, provides a means for the model to <i>explain itself</i> in terms we already understand.
                        </p>

                        <p style="font-size: 0.7em";>* The examples below, although accurate in some instances, still contain many errors between the images and caption.
                            The interface is primarily an  example of what future interpretability / knowledge interfaces could look like and be used for.</p>
                    </div>

                    <div id="map">
                        <div class="map-canvas"></div>
                        <div id="overlay"></div>
                    </div>


                    <div id="display">
                        <img alt="" id="interp" width="256" height="256" src="./imgs/grid/50_50.jpg"/>
                    </div>

                    <div id="grid">
                        <img  width="256" height="256" src="./imgs/grid_black.jpg"/>
                        <div id="loc"></div>
                    </div>

                    <div id="annotations">
                        <p id="anno_content"></p>
                    </div>


                </div>

                <script src="./grid_annos.js"></script>
                <script src="./script.js"></script>

            </section>


            <section id="references" style="max-width: 700px; margin: auto;">


                <h1>References</h1>

                <p>Lewis Jr, J. S., Tarabishy, Y., Luo, J., Mani, H., Bishop, J. A., Leon, M. E., ... & Di Palma, S. (2015). Inter-and intra-observer variability in the classification of extracapsular extension in p16 positive oropharyngeal squamous cell carcinoma nodal metastases. Oral oncology, 51(11), 985-990.</p>

                <p>Wu, Z., Lischinski, D., & Shechtman, E. (2021). Stylespace analysis: Disentangled controls for stylegan image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12863-12872). </p>

            </section>

        </div>
    </body>
</html>




